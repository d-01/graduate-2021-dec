{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_demo_kaggle-submission-generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymB0eI9MJOXr"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_BjPx_hJNof"
      },
      "source": [
        "SAVED_MODEL_GDRIVE_ID = '139hpFUxh9toyU0LwDq7mibYr6HjJgh6f'\n",
        "SAVED_MODEL_FILE_PATH = 'saved_model_infr_fer.h5'\n",
        "TEST_KAGGLE_GDRIVE_ID = '1bGHeWeWYXj5biL9s-qTc9gyv91WNAbWE'\n",
        "TEST_KAGGLE_DIRECTORY = 'test_kaggle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ZNPpKR24LN"
      },
      "source": [
        "## Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWvS86Y662yl",
        "outputId": "62952b73-a01b-42db-8703-246775df5104"
      },
      "source": [
        "!gdown -O {SAVED_MODEL_FILE_PATH} --id {SAVED_MODEL_GDRIVE_ID}\n",
        "!gdown -O {TEST_KAGGLE_DIRECTORY}.zip --id {TEST_KAGGLE_GDRIVE_ID}\n",
        "\n",
        "!unzip -q {TEST_KAGGLE_DIRECTORY}.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=139hpFUxh9toyU0LwDq7mibYr6HjJgh6f\n",
            "To: /content/saved_model_infr_fer.h5\n",
            "100% 1.19M/1.19M [00:00<00:00, 45.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bGHeWeWYXj5biL9s-qTc9gyv91WNAbWE\n",
            "To: /content/test_kaggle.zip\n",
            "100% 222M/222M [00:01<00:00, 112MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2B41Y_stzMg"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "assert Path(SAVED_MODEL_FILE_PATH).is_file(), f'\"{SAVED_MODEL_FILE_PATH}\" file is required'\n",
        "assert Path(TEST_KAGGLE_DIRECTORY).is_dir(), f'\"{TEST_KAGGLE_DIRECTORY}\" directory is required'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEWUE9ch26mF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUp2ReEI6V4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0707ff1-f08e-4949-abe7-e67937abb9b2"
      },
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "/print tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRzeX5z53AKB"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dws7W_DY-lxa",
        "cellView": "form"
      },
      "source": [
        "#@title Functions { form-width: \"1px\" }\n",
        "#@markdown ```python\n",
        "#@markdown def read_images(paths, size=None, mode='rgb', return_skipped=False)\n",
        "#@markdown ```\n",
        "def read_images(paths, size=None, mode='rgb', return_skipped=False):\n",
        "    \"\"\"Given list of image file paths, read image date to uint8 numpy arrays.\n",
        "\n",
        "    Output image formats:\n",
        "    * rgb, shape=(h, w, 3), color channels: R, G, B\n",
        "        * Common image format for color images\n",
        "    * bgr, shape=(h, w, 3), color channels: B, G, R\n",
        "        * OpenCV format for color images\n",
        "    * gray, shape=(h, w), color channels: none\n",
        "        * Common format (PIL, matplotlib, opencv) for grayscale images\n",
        "    * gray_1ch, shape=(h, w, 1), color channels: MONO\n",
        "        * TensorFlow format for grayscale images\n",
        "\n",
        "    paths : list of str\n",
        "        List of paths. Note: `cv2.imread()` not working with `pathlib.Path()`,\n",
        "        str required.\n",
        "    size : None or tuple(int, int)\n",
        "        If None -- don't risize images. Else must be a destination size as\n",
        "        tuple(width: int, height: int).\n",
        "    mode : {'rgb', 'bgr', 'gray', 'gray_1ch'}\n",
        "        Output image format.\n",
        "    return_skipped : bool\n",
        "        Return a list of paths failed to read (doesn't exist, bad format, no access, etc.).\n",
        "\n",
        "    Returns:\n",
        "        Images as uint8 numpy arrays.\n",
        "        * If size=None: list of non-uniformly sized arrays (\"ragged array\").\n",
        "        * If size is not None: batch of images as ndarray, where the first axis (axis=0)\n",
        "          equals to the number of read images.\n",
        "        * If mode='gray', each image represented as a 2D array (h, w),\n",
        "          else as a 3D array (h, w, c).\n",
        "        * If skipped_files=True: tuple where second element is a list of skipped\n",
        "          paths.\n",
        "\n",
        "    Example, without resize (size=None):\n",
        "\n",
        "    >>> [img.shape for img in read_images(paths[:3], size=None)]\n",
        "    [(201, 201, 3), (231, 231, 3), (296, 296, 3)]\n",
        "\n",
        "    Example, with resize:\n",
        "\n",
        "    >>> imgs = read_images(paths[:2], size=(100, 50))\n",
        "    >>> imgs.shape\n",
        "    (2, 50, 100, 3)\n",
        "    \"\"\"\n",
        "    assert mode in ['rgb', 'bgr', 'gray', 'gray_1ch']\n",
        "\n",
        "    n = len(paths)\n",
        "    skipped_files = []\n",
        "\n",
        "    if size is None:\n",
        "        imgs = [None] * n\n",
        "    else:\n",
        "        w, h = size\n",
        "        if mode == 'gray':\n",
        "            imgs = np.zeros((n, h, w), dtype=np.uint8)\n",
        "        elif mode == 'gray_1ch':\n",
        "            imgs = np.zeros((n, h, w, 1), dtype=np.uint8)\n",
        "        else:\n",
        "            imgs = np.zeros((n, h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    i = 0\n",
        "    for path in tqdm(paths):\n",
        "        if mode in ['rgb', 'bgr']:\n",
        "            img = cv2.imread(path, cv2.IMREAD_COLOR)      # (h, w, 3)\n",
        "        elif mode in ['gray', 'gray_1ch']:\n",
        "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # (h, w)\n",
        "        if img is None:\n",
        "            skipped_files.append(path)\n",
        "            continue\n",
        "        if mode == 'rgb':\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if size is not None:\n",
        "            if sum(img.shape[:2]) > sum(size):\n",
        "                # Current size > Target size\n",
        "                img = cv2.resize(img, (w, h), cv2.INTER_AREA)  # downscale\n",
        "            else:\n",
        "                img = cv2.resize(img, (w, h), cv2.INTER_LINEAR)  # upscale\n",
        "        if mode == 'gray_1ch':\n",
        "            img = img[..., None]  # (h, w) -> (h, w, 1)\n",
        "\n",
        "        imgs[i] = img\n",
        "        i += 1\n",
        "    \n",
        "    if i < n:\n",
        "        print(f\"{i}/{n} files have been read\")\n",
        "        imgs = imgs[:i]\n",
        "    \n",
        "    if return_skipped:\n",
        "        return (imgs, skipped_files)\n",
        "    else:\n",
        "        return imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZMZ_IdSbw6-I"
      },
      "source": [
        "#@title Classes { form-width: \"1px\" }\n",
        "#@markdown ```python\n",
        "#@markdown class ModelFER(saved_model_path)\n",
        "#@markdown ```\n",
        "class ModelFER:\n",
        "    \"\"\"Model for Facial Expression Recognition\n",
        "    \n",
        "    Classifies given image(s) of a face into one of the 9 categories: anger,\n",
        "    contempt, disgust, fear, happy, neutral, sad, surprise and uncertain.\n",
        "\n",
        "    Input format:\n",
        "        * dtype=uint8\n",
        "        * shape\n",
        "            * (h, w, c) -- single image input for `predict()`\n",
        "            * (b, h, w, c) -- (b=batch_size) batch input for `predict_batch()`\n",
        "        * c=channels\n",
        "            * c=1 -- for grayscale images\n",
        "            * c=3 -- for RGB images\n",
        "\n",
        "    Grayscale images without channel dimention is not supported, i.e. arrays\n",
        "    with shapes (h, w) or (b, h, w) are not allowed.\n",
        "    \"\"\"\n",
        "    def __init__(self, saved_model_path):\n",
        "        self.tf_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(images):\n",
        "        \"\"\"Preprocess batch of images for input.\n",
        "        \n",
        "        Valid input:\n",
        "        1. shape: (b, h, w, {1, 3})\n",
        "        2. dtype: uint8 or float32\n",
        "        3. values range: [0, 255]\n",
        "\n",
        "        Output:\n",
        "        1. shape: (b, 128, 128, 1)\n",
        "        2. dtype: uint8\n",
        "        3. values range: [0, 255]\n",
        "        \"\"\"\n",
        "        # Assert input shape\n",
        "        n_channels = images.shape[-1]\n",
        "        if images.ndim == 4 and n_channels in [1, 3]:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'Expected `images.shape` in [(b, h, w, 3), (b, h, w, 1)],'\n",
        "                f' got: {images.shape}')\n",
        "\n",
        "        raw_images = images\n",
        "\n",
        "        # RGB to grayscale if needed: (b, h, w, 3) -> (b, h, w, 1)\n",
        "        if n_channels == 3:\n",
        "            images = tf.image.rgb_to_grayscale(images).numpy()\n",
        "\n",
        "        # Resize if needed\n",
        "        (in_h, in_w, in_c) = (128, 128, 1)  # model_input_size -- can be read from `model.input`\n",
        "        (b, h, w, c) = images.shape  # actual size\n",
        "\n",
        "        if in_h != h or in_w != w:\n",
        "            images = tf.image.resize(images, (in_h, in_w)).numpy()  # returns float32\n",
        "            images = images.astype(np.uint8)                        # float32 -> uint8\n",
        "\n",
        "        assert (images.shape == (b, in_h, in_w, in_c)\n",
        "                and images.dtype == np.uint8), 'This function has a bug. Check the code.'\n",
        "        return images\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def decode_preds(preds):\n",
        "        class_names = np.array(['anger', 'contempt', 'disgust', 'fear', 'happy',\n",
        "                             'neutral', 'sad', 'surprise', 'uncertain'])\n",
        "        return class_names[preds.argmax(axis=1)]\n",
        "\n",
        "\n",
        "    def predict_batch(self, images):\n",
        "        \"\"\"Convinience function that: (1) verifies format and prepares images for\n",
        "            inference, (2) calls `tf_model.predict()`, (3) decodes predictions.\n",
        "\n",
        "        images : array\n",
        "            Batch of images in form of 4D uint8 array.\n",
        "        \"\"\"\n",
        "        assert images.dtype == np.uint8, f'got {images.dtype}'\n",
        "\n",
        "        if images.ndim == 3:\n",
        "            raise ValueError(\n",
        "                f'Expected batch input, got: {images.shape}. For single image'\n",
        "                f' input use `predict()` method.')\n",
        "        raw_images = images\n",
        "        images = self.preprocess(raw_images)\n",
        "\n",
        "        preds = self.tf_model.predict(images)\n",
        "        classes = self.decode_preds(preds)\n",
        "        return classes\n",
        "\n",
        "\n",
        "    def predict(self, image):\n",
        "        \"\"\"\n",
        "        image : array\n",
        "            Gray or RGB 8bit image in form of 2D or 3D uint8 array.\n",
        "        \"\"\"\n",
        "        assert image.dtype == np.uint8, f'got {image.dtype}'\n",
        "\n",
        "        if image.ndim == 4:\n",
        "            raise ValueError(\n",
        "                f'Expected single image input, got: {image.shape}. For batch'\n",
        "                f' input use `predict_batch()` method.')\n",
        "        batch = image[None, ...]\n",
        "        class_name, = self.predict_batch(batch)\n",
        "        return class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcyjCsAHGwvL"
      },
      "source": [
        "## Read test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JmGHS5XTv5l",
        "outputId": "711cad56-b355-4ba5-a9ea-c88ab687695d"
      },
      "source": [
        "# Create paths list\n",
        "test_filenames = [f'{i}.jpg' for i in range(5000)]\n",
        "paths_test = [f'{TEST_KAGGLE_DIRECTORY}/{filename}' for filename in test_filenames]\n",
        "paths_test[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test_kaggle/0.jpg', 'test_kaggle/1.jpg', 'test_kaggle/2.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSNA02XHavER",
        "outputId": "e3caf91a-41c6-4af6-a28a-f1196462c5ea"
      },
      "source": [
        "# Load images in RAM\n",
        "imgs_test = read_images(paths_test, size=(128, 128), mode='gray_1ch')\n",
        "imgs_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:14<00:00, 347.07it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 128, 128, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFJuMkl2b8J"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WkmC1uiJhVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19da2a1b-4a3e-4f0a-ff23-5e66281a977d"
      },
      "source": [
        "model = ModelFER(SAVED_MODEL_FILE_PATH)\n",
        "model.tf_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
            "                                                                 \n",
            " tf.image.grayscale_to_rgb_3  (None, 128, 128, 3)      0         \n",
            "  (TFOpLambda)                                                   \n",
            "                                                                 \n",
            " tf.cast_3 (TFOpLambda)      (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " tf.math.truediv_3 (TFOpLamb  (None, 128, 128, 3)      0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " tf.math.subtract_3 (TFOpLam  (None, 128, 128, 3)      0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " mobilenet_0.25_128 (Functio  (None, 4, 4, 256)        218544    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 36873     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,417\n",
            "Trainable params: 249,945\n",
            "Non-trainable params: 5,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7MWB4xN2fsN"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxhUlmSHVTTO",
        "outputId": "07d0437b-5ffd-4458-e882-c777caaab643"
      },
      "source": [
        "predicted_classes = model.predict_batch(imgs_test)\n",
        "len(predicted_classes), predicted_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, array(['sad', 'neutral', 'sad', ..., 'neutral', 'sad', 'happy'],\n",
              "       dtype='<U9'))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WDUv3Yj1sYY"
      },
      "source": [
        "## Write submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL6QkJk3VTGB",
        "outputId": "6e81b98d-7932-4e74-bb79-0e6a34b40a9c"
      },
      "source": [
        "# submission.csv\n",
        "rows = ['image_path,emotion']\n",
        "rows.extend(map('{},{}'.format, test_filenames, predicted_classes))\n",
        "csv_content = '\\r\\n'.join(rows) + '\\r\\n'\n",
        "\n",
        "Path('submission.csv').write_text(csv_content, encoding='utf8')\n",
        "\n",
        "# kaggle score (private/public): 0.46440 / 0.46640"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82816"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCD3NGv_2QEG"
      },
      "source": [
        "## Measure inference time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9KX-ua63K0o"
      },
      "source": [
        "def measure_performance(tf_model, batches, how_long_s=None):\n",
        "    t0 = time.time()\n",
        "    for frame, batch in enumerate(batches, start=1):\n",
        "        tf_model(batch, training=False)\n",
        "        if time.time() - t0 > how_long_s:\n",
        "            break\n",
        "\n",
        "    sec = time.time() - t0\n",
        "    fps = frame / sec\n",
        "    print(f'{1/fps:.3f}s/img ({fps:.0f}fps)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC0UlP9E6O4R",
        "outputId": "66af240c-5732-447e-b5a4-0fb01d107d26"
      },
      "source": [
        "# 1-sized batches\n",
        "dummy_data = np.random.RandomState(0).randint(0, 256, (1000, 1, 128, 128, 1), dtype=np.uint8)\n",
        "tf_model = model.tf_model\n",
        "\n",
        "# Warmup\n",
        "tf_model(dummy_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 9), dtype=float32, numpy=\n",
              "array([[-1.0647646 , -2.385496  , -2.0123975 , -0.36399448,  1.6006172 ,\n",
              "         1.3274012 ,  1.2064764 ,  0.94273823,  0.26824668]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K40oCt6qxjJ"
      },
      "source": [
        "### CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2LEuKPCBebb",
        "outputId": "475fa367-13d3-4904-8e1b-ddcde52b26e0"
      },
      "source": [
        "#@title Hardware report { display-mode: \"form\" }\n",
        "def report_hardware():\n",
        "    split_space = lambda s: list(filter(None, s.split()))\n",
        "\n",
        "    # CPU\n",
        "    cpu_info = !cat /proc/cpuinfo\n",
        "    print('CPU: {}x {} @ cache {}'.format(len(cpu_info)//27, cpu_info[4][13:], cpu_info[8][13:]))\n",
        "\n",
        "    # RAM\n",
        "    mem_info = !cat /proc/meminfo\n",
        "    mem_total_gb = int(split_space(mem_info[0])[1]) / 1024 / 1024\n",
        "    mem_avail_gb = int(split_space(mem_info[2])[1]) / 1024 / 1024\n",
        "    print('RAM: {0:.1f} GB total, {1:.1f} GB avail'.format(mem_total_gb, mem_avail_gb))\n",
        "\n",
        "    # GPU\n",
        "    from tensorflow.python.client import device_lib\n",
        "    has_gpu = False\n",
        "    for device in device_lib.list_local_devices():\n",
        "        if device.device_type == 'GPU':\n",
        "            has_gpu = True\n",
        "            device_specs = dict(item.split(': ') for item in device.physical_device_desc.split(', '))\n",
        "            print('GPU: {0} (name: \"{1}\", compute capability: {2}, memory: {3:.1f}GB)'.format(\n",
        "                device_specs['name'],\n",
        "                device.name,\n",
        "                device_specs['compute capability'],\n",
        "                device.memory_limit / 1024 / 1024 / 1024))\n",
        "    if not has_gpu:\n",
        "        print('GPU: none')\n",
        "\n",
        "    # Disk\n",
        "    disc_info = !df -h /content\n",
        "    print('Disk: {1} total, {4} used, {3} avail'.format(*split_space(disc_info[1])))\n",
        "\n",
        "report_hardware()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 2x Intel(R) Xeon(R) CPU @ 2.20GHz @ cache 56320 KB\n",
            "RAM: 12.7 GB total, 11.9 GB avail\n",
            "GPU: none\n",
            "Disk: 108G total, 39% used, 67G avail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ82T0eh3SWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab26e706-b0b9-4600-e0ba-ffc341f246b5"
      },
      "source": [
        "measure_performance(tf_model, dummy_data, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.037s/img (27fps)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dywBOTjuqzkP"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRThnC1O3I5W",
        "outputId": "4ba1faa4-b3ed-4e22-dfec-5f78f7ec0890"
      },
      "source": [
        "#@title Hardware report { display-mode: \"form\" }\n",
        "def report_hardware():\n",
        "    split_space = lambda s: list(filter(None, s.split()))\n",
        "\n",
        "    # CPU\n",
        "    cpu_info = !cat /proc/cpuinfo\n",
        "    print('CPU: {}x {} @ cache {}'.format(len(cpu_info)//27, cpu_info[4][13:], cpu_info[8][13:]))\n",
        "\n",
        "    # RAM\n",
        "    mem_info = !cat /proc/meminfo\n",
        "    mem_total_gb = int(split_space(mem_info[0])[1]) / 1024 / 1024\n",
        "    mem_avail_gb = int(split_space(mem_info[2])[1]) / 1024 / 1024\n",
        "    print('RAM: {0:.1f} GB total, {1:.1f} GB avail'.format(mem_total_gb, mem_avail_gb))\n",
        "\n",
        "    # GPU\n",
        "    from tensorflow.python.client import device_lib\n",
        "    has_gpu = False\n",
        "    for device in device_lib.list_local_devices():\n",
        "        if device.device_type == 'GPU':\n",
        "            has_gpu = True\n",
        "            device_specs = dict(item.split(': ') for item in device.physical_device_desc.split(', '))\n",
        "            print('GPU: {0} (name: \"{1}\", compute capability: {2}, memory: {3:.1f}GB)'.format(\n",
        "                device_specs['name'],\n",
        "                device.name,\n",
        "                device_specs['compute capability'],\n",
        "                device.memory_limit / 1024 / 1024 / 1024))\n",
        "    if not has_gpu:\n",
        "        print('GPU: none')\n",
        "\n",
        "    # Disk\n",
        "    disc_info = !df -h /content\n",
        "    print('Disk: {1} total, {4} used, {3} avail'.format(*split_space(disc_info[1])))\n",
        "\n",
        "report_hardware()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 2x Intel(R) Xeon(R) CPU @ 2.30GHz @ cache 46080 KB\n",
            "RAM: 12.7 GB total, 11.0 GB avail\n",
            "GPU: Tesla K80 (name: \"/device:GPU:0\", compute capability: 3.7, memory: 10.5GB)\n",
            "Disk: 79G total, 55% used, 36G avail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT0q-AWlF6FX",
        "outputId": "aa0e3ad9-a52b-446f-e499-711988bc8551"
      },
      "source": [
        "measure_performance(tf_model, dummy_data, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.042s/img (24fps)\n"
          ]
        }
      ]
    }
  ]
}